name: prometheus-monitoring-services

services:
  prometheus:
    image: prom/prometheus:latest
    volumes:
      # Mount the local prometheus.yml file into the container's config path.
      - ./prometheus.yml:/etc/prometheus/prometheus.yml 
      # Optional: Persist Prometheus data on the host to retain metrics.
      # - ./prometheus_data:/prometheus
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=2d # 90일보다 오래된 metrics는 삭제
      - --storage.tsdb.retention.size=1GB # 10GB를 넘을 시 오래된 metrics 삭제
      - --web.console.libraries=/usr/share/prometheus/console_libraries
      - --web.console.templates=/usr/share/proemtheus/consoles
      - --web.enable-admin-api
    ports:
      - '9090:9090'
    networks:
      - bridge

  # docker run -d --name clickhouse-server -p 8123:8123 -p 9000:9000 clickhouse/clickhouse-server
  # docker run --restart always -d -p 8123:8123 -p 9000:9000 -v clickhouse:/var/lib/clickhouse -e CLICKHOUSE_DB=test --name clickhouse --ulimit nofile=262144:262144 clickhouse/clickhouse-server
  clickhouse-server:
    image: clickhouse/clickhouse-server:latest
    ports:
      - "8123:8123"
      - "9000:9000"
      - "9009:9009"
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - clickhouse_logs:/var/log/clickhouse-server
      - ./config/users.xml:/etc/clickhouse-server/users.d/users.xml
      - ./config/config.xml:/etc/clickhouse-server/config.d/config.xml
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    networks:
      - bridge

  clickhouse-client:
    image: clickhouse/clickhouse-client:latest
    networks:
      - bridge
    # Use 'command: ...' to connect to the server service upon startup if needed,
    # or run interactively with 'docker compose run clickhouse-client'
    command: bash -c "sleep 5 && clickhouse-client --host clickhouse-server"
    depends_on:
      clickhouse-server:
        condition: service_healthy # Ensure the server is ready before client connects

  # docker run -d -p 9091:9091 prom/pushgateway
  pushgateway:
    image: prom/pushgateway:latest
    ports:
      - "9091:9091"
    restart: always
    networks:
      - bridge

  nginx:
    image: nginx:alpine
    container_name: nginx_proxy
    ports:
      # Expose the Nginx port to the host machine (e.g., 8080)
      - "8080:80"
    volumes:
      # Mount the Nginx config and htpasswd file into the container
      - ./nginx_conf/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx_conf/.htpasswd:/etc/nginx/.htpasswd:ro
    depends_on:
      - pushgateway  

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    networks:
      - bridge
    depends_on:
      - prometheus

  blackbox-exporter:
    image: prom/blackbox-exporter:latest
    container_name: blackbox_exporter
    restart: unless-stopped
    ports:
      - "9115:9115"
    volumes:
      # Mount the local blackbox.yml to the container's config path
      - ./config-prometheus-blackbox.yml:/etc/blackbox/blackbox.yml:ro
    command:
      - '--config.file=/etc/blackbox/blackbox.yml'
    networks:
      - bridge

  # docker run --rm -p 9100:9100 prom/node-exporter 
  # docker compose up -d node-exporter
  # https://github.com/prometheus/node_exporter/
  node_exporter:
    # http://localhost:9100/metrics
    image: prom/node-exporter
    container_name: node_exporter
    depends_on:
      - prometheus
    restart: always
    networks:
      - bridge
    ports:
      - 9100:9100

  # To run the Jaeger query service using Docker Compose, the most common approach is to use the all-in-one image, which includes the agent, collector, query service, and UI in a single container for testing and development
  # Jaeger's All-in-One mode uses in-memory storage by default.
  # docker run -d --name jaeger -p 16686:16686 -p 4317:4317 -p 4318:4318 cr.jaegertracing.io/jaegertracing/jaeger:latest
  # docker run -d --name jaeger-es -p 16686:16686 -p 14250:14250 -p 14268:14268 -v ./config-elasticsearch.yaml:/config.yaml jaegertracing/all-in-one:latest --config-file=/config.yaml
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    ports:
      # UDP ports for the Jaeger agent (jaeger.thrift)
      - "6831:6831/udp" # Compact Thrift protocol
      - "6832:6832/udp" # Binary Thrift protocol

      # Other useful Jaeger ports (optional)
      # - "16686:16686" # Jaeger UI
      # - "14250:14250" # gRPC port for Jaeger collector
      # - "14268:14268" # HTTP port for spans

      # OTLP gRPC receiver port
      - "4317:4317"
      # OTLP HTTP receiver port
      - "4318:4318"
      # Jaeger UI port
      - '16686:16686'
    # restart: always
    restart: on-failure
    environment:
      # Enable OTLP reception
      - COLLECTOR_OTLP_ENABLED=true
      - LOG_LEVEL=debug # Optional: for more detailed logs
      # Set the storage type to in-memory for the all-in-one image
      # - SPAN_STORAGE_TYPE=memory
      # - SPAN_STORAGE_TYPE=elasticsearch
      # - ES_SERVER_URLS=http://elasticsearch:9200
    networks:
      - bridge

  hotrod:
    image: jaegertracing/example-hotrod:latest
    # To run the latest trunk build, find the tag at Docker Hub and use the line below
    # https://hub.docker.com/r/jaegertracing/example-hotrod-snapshot/tags
    #image: jaegertracing/example-hotrod-snapshot:0ab8f2fcb12ff0d10830c1ee3bb52b745522db6c
    ports:
      - "8080:8080"
      - "8083:8083"
    command: ["all"]
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4318
    networks:
      - bridge
    depends_on:
      - jaeger

  # This creates rabbit and opens the port.
  #docker run -d --hostname my-rabbit --name some-rabbit -p 5672:5672 -p 8080:15672 rabbitmq:3
  # Creates the RabbitMQ UI management and opens to port 9000   Then you can do http://localhost:9000/#/ to see rabbit working.
  #docker run -d --hostname my-rabbit --name some-rabbit-management -p 9000:15672 rabbitmq:3-management
  rabbitmq:
    image: rabbitmq:3-management
    container_name: rabbitmq
    ports:
      - 5672:5672
      - 15672:15672
    networks:
      - bridge

  influxdb:
    image: influxdb:1.8-alpine
    container_name: influxdb
    restart: always
    environment:
      - INFLUXDB_DB=influx
      - INFLUXDB_ADMIN_USER=admin
      - INFLUXDB_ADMIN_PASSWORD=admin
    ports:
      - '8086:8086'  # Expose InfluxDB on port 8086
    volumes:
      - influxdb_data:/var/lib/influxdb  # Mount the InfluxDB data directory
    networks:
      - bridge  # Connect the InfluxDB container to the monitoring network

  # https://velog.io/@leesjpr/Grafana-InfluxDB2-Telegraf-Monitoring
  telegraf:
    image: telegraf
    container_name: telegraf
    restart: always
    volumes:
      - ./telegraf/telegraf.conf:/etc/telegraf/telegraf.conf:ro  # Mount the Telegraf configuration file
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Mount the Docker socket to collect Docker metrics
    depends_on:
      - influxdb
      # - nginx
    links:
      - influxdb
      # - nginx
    ports:
      - '8125:8125'  # Expose Telegraf's StatsD port
    networks:
      - bridge  # Connect the Telegraf container to the monitoring network

  telegraf_second:
    image: telegraf
    container_name: telegraf_second
    restart: always
    volumes:
      - ./telegraf/telegraf_second.conf:/etc/telegraf/telegraf.conf:ro  # Mount the second Telegraf configuration file
    depends_on:
      - influxdb
    links:
      - influxdb
    ports:
      - '8126:8126'  # Expose the second Telegraf's StatsD port
    networks:
      - bridge  # Connect the second Telegraf container to the monitoring network

volumes:
  influxdb_data: {}  # Volume for InfluxDB data
  clickhouse_data: {}
  clickhouse_logs: {}

networks:
  bridge:
    driver: bridge