{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a176f610-cf34-41d3-902d-cac996cb5878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the records if index has duplicate recores\n"
     ]
    }
   ],
   "source": [
    "''' Jupyter Notebook is an incredibly powerful tool for interactively developing and presenting data science projects.  '''\n",
    "''' It allows you to run code, display the results, and add explanations, formulas, and charts all in one place.  '''\n",
    "\n",
    "''' Script that can check for duplicate entries for TASK documents, it would be great if we could do this for all WMx/OMx ES indices.'''\n",
    "''' Modify your script to check for any process name by the below field(s) to see if any duplicate data exists. '''\n",
    "print(f\"Check the records if index has duplicate recores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfa7a8d7-7409-4020-a4e2-ec8b907c6670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' To begin, you need to install pytest and ipytest, a tool designed to run pytest tests directly in Jupyter. Execute the following in a Jupyter cell '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip uninstall -y elasticsearch\n",
    "#!pip install elasticsearch==7.13.0\n",
    "''' To begin, you need to install pytest and ipytest, a tool designed to run pytest tests directly in Jupyter. Execute the following in a Jupyter cell '''\n",
    "# !pip install pytest ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a58b73c-bd80-4906-b7df-57e3c87c6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c8a2d0-6fea-47f6-aa47-7272e41717ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: elasticsearch\n",
      "Version: 7.13.0\n",
      "Summary: Python client for Elasticsearch\n",
      "Home-page: https://github.com/elastic/elasticsearch-py\n",
      "Author: Honza KrÃ¡l, Nick Lang\n",
      "Author-email: honza.kral@gmail.com, nick@nicklang.com\n",
      "License: Apache-2.0\n",
      "Location: /home/biadmin/monitoring/jupyter_notebook/.venv/lib/python3.9/site-packages\n",
      "Requires: certifi, urllib3\n",
      "Required-by: \n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip show elasticsearch\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f63ef6e-8aec-4897-a230-d2ff40ea3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "# import pytest\n",
    "import ipytest\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86f106ca-c5b2-4907-aade-b7f6b81fefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' To begin, you need to install pytest and ipytest, a tool designed to run pytest tests directly in Jupyter. Execute the following in a Jupyter cell '''\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df8cea7f-c5ae-4cab-ae5d-9d7a272a35e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' pip install python-dotenv'''\n",
    "load_dotenv() # will search for .env file in local folder and load variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fcd93bf-191a-4bff-876f-010802bf629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers():\n",
    "    ''' Elasticsearch Header '''\n",
    "    return {\n",
    "            'Content-type': 'application/json', \n",
    "            'Authorization' : '{}'.format(os.getenv('BASIC_AUTH')),\n",
    "            'Connection': 'close'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d9a80da-4712-41ef-b5db-a1aea3e96f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_es_instance(host):\n",
    "    es_client = Elasticsearch(hosts=\"http://{}\".format(host), headers=get_headers(), timeout=60,  verify_certs=False)\n",
    "    return es_client\n",
    "\n",
    "# es_obj_s_client = get_es_instance(\"localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0238edd-55c4-4f4d-8ac6-43233b143bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp = es_obj_s_client.cluster.health()\n",
    "# print(json.dumps(resp, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62db2a74-e4c0-4f5b-8a79-e44cea9f050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_host_duplicates = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a986a0be-0f8c-446e-8b07-f0c43871a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_value_to_transform_trim(raw_json):\n",
    "    ''' update value in the form of json format'''\n",
    "    # print(f\"raw_json : {raw_json}\")\n",
    "    def get_recursive_nested_all(d):\n",
    "        # print(f\"get_recursive_nested_all : {d}\")\n",
    "        if isinstance(d, list):\n",
    "            for i in d:\n",
    "                get_recursive_nested_all(i)\n",
    "        elif isinstance(d, dict):\n",
    "            for k, v in d.items():\n",
    "                if not isinstance(v, (list, dict)):\n",
    "                    # print(\"%%%%\", k, v)\n",
    "                    d[k] = v\n",
    "                else:\n",
    "                    # print(\"####\", k, v)\n",
    "                    get_recursive_nested_all(v)\n",
    "        return d\n",
    "\n",
    "    return get_recursive_nested_all(raw_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4117f1e6-1dbc-4e31-bef4-d351d6ee9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def script_query_to_query_list(script_query) -> list:\n",
    "    ''' Validate script query when transforming to multiple fields '''\n",
    "    # script_query = \"doc['RECEIPTKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\"\n",
    "    # script_query = \"doc['COMPANYKEY.keyword'].value + params.param + doc['COMPANYTYPE.keyword'].value + params.param + doc['CLIENTID.keyword'].value + params.param + doc['SITEID.keyword'].value\"\n",
    "    script_query = script_query.replace('.value + params.param ', '')\n",
    "    script_query = script_query.replace('].value ', '')\n",
    "    script_query = script_query.replace('.value', '')\n",
    "    script_query = script_query.replace('doc', '')\n",
    "    script_query = script_query.replace(\"'\", '')\n",
    "    script_query = script_query.replace(\"]\", '')\n",
    "    script_query = script_query.replace(\"+\", '')\n",
    "    script_query = script_query.split('[')\n",
    "    script_query = [str(strs).strip() for strs in script_query if len(strs) >0]\n",
    "    # print(script_query)\n",
    "    return script_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3464c3fc-69d0-43c3-9f54-ab0e77e238ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_handler(process, script_query, basic_query=True):\n",
    "    ''' check dsl for any duplicates '''\n",
    "    '''https://stackoverflow.com/questions/53076349/script-writing-to-get-distinct-value-from-elasticsearch '''\n",
    "    ''' https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-script '''\n",
    "    ''' So you can see in the results as how key are constructed (keys are unique). '''\n",
    "    '''\n",
    "    {\n",
    "      \"size\": 0,\n",
    "      \"aggs\": {\n",
    "        \"duplicates\": {\n",
    "          \"terms\": {\n",
    "            \"script\": {\n",
    "              #\"source\": \"doc['TASKID'].value\",\n",
    "              \"source\": \"doc['ORDERKEY.keyword'].value + params.param + doc['ORDERLINENO'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "              \"lang\": \"painless\",\n",
    "              \"params\": {\n",
    "                \"param\": \",\"\n",
    "              }\n",
    "            },\n",
    "            \"min_doc_count\": 2,\n",
    "            \"size\": 10000\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    with_aggs_query = ['wx_inv_trans', 'wx_order', 'wx_order_casemnf', 'om_inventorytransaction', 'om_mbol', 'om_receipt']\n",
    "    if basic_query:\n",
    "        ''' basic aggs query to avoid timeout during running the query '''\n",
    "        if process in with_aggs_query:\n",
    "            ''' It requires that script_query_to_query_list has two element from the function '''\n",
    "            aggs_query = script_query_to_query_list(script_query)\n",
    "            return process, {\n",
    "                  \"size\": 0,\n",
    "                  \"aggs\": {\n",
    "                    \"duplicates\": {\n",
    "                      \"terms\": {\n",
    "                        \"field\": aggs_query[0],\n",
    "                        \"min_doc_count\": 2\n",
    "                      },\n",
    "                      \"aggs\": {\n",
    "                        \"sub\": {\n",
    "                          \"terms\": {\n",
    "                            \"field\": aggs_query[1],\n",
    "                            \"min_doc_count\": 2,\n",
    "                            \"size\": 10000\n",
    "                          }\n",
    "                        },\n",
    "                        \"min_bucket_selector\": {\n",
    "                          \"bucket_selector\": {\n",
    "                            \"buckets_path\": {\n",
    "                              \"count\": \"sub._bucket_count\"\n",
    "                            },\n",
    "                            \"script\": {\n",
    "                              \"source\": \"params.count > 0\"\n",
    "                            }\n",
    "                          }\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "        else:\n",
    "            query = {\n",
    "                  \"size\": 0,\n",
    "                  \"aggs\": {\n",
    "                    \"duplicates\": {\n",
    "                      \"terms\": {\n",
    "                        \"script\": {\n",
    "                          # \"source\": \"doc['TASKID'].value\",\n",
    "                          \"source\": script_query,\n",
    "                          \"lang\": \"painless\",\n",
    "                          \"params\": {\n",
    "                            \"param\": \",\"\n",
    "                          }\n",
    "                        },\n",
    "                        \"min_doc_count\": 2,\n",
    "                        \"size\" : 10000\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "    else:\n",
    "        query = {\n",
    "          \"size\": 0,\n",
    "          \"aggs\": {\n",
    "            \"duplicates\": {\n",
    "              \"terms\": {\n",
    "                \"script\": {\n",
    "                  # \"source\": \"doc['TASKID'].value\",\n",
    "                  \"source\": script_query,\n",
    "                  \"lang\": \"painless\",\n",
    "                  \"params\": {\n",
    "                    \"param\": \",\"\n",
    "                  }\n",
    "                },\n",
    "                \"min_doc_count\": 2,\n",
    "                \"size\" : 10000\n",
    "              },\n",
    "              \"aggs\": {\n",
    "                \"first-report\": {\n",
    "                  \"top_hits\": {\n",
    "                    \"_source\": [\"_id\"], \n",
    "                    \"size\": 10\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    # if process == 'sample':\n",
    "        # query = {\n",
    "        #   \"size\": 0,\n",
    "        #   \"aggs\": {\n",
    "        #     \"duplicates\": {\n",
    "        #       \"terms\": {\n",
    "        #             \"field\": \"TASKID\",\n",
    "        #             \"min_doc_count\": 2,\n",
    "        #             \"size\" : 10000  \n",
    "        #         }\n",
    "        #      }\n",
    "        #   }\n",
    "        # }\n",
    "\n",
    "    # print(json.dumps(query, indent=2))\n",
    "    # exit(1)\n",
    "    \n",
    "    return process, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80a63282-6377-4ab8-8a68-4bc89251e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_tasks(env, process_script_query_dict, process):\n",
    "    ''' check duplicates'''\n",
    "\n",
    "    try:\n",
    "        # print('\\n', env)\n",
    "        # print(os.getenv(\"{}_ES_HOST\".format(str(env).upper())))\n",
    "        es_host = \"{}_ES_HOST\".format(str(env).upper())\n",
    "        # dataframe_column.append(es_host)\n",
    "        dataframe_column.append(str(env).upper())\n",
    "        dataframe_process.append(process)\n",
    "    \n",
    "        if 'wx' in process:\n",
    "            dataframe_db.append('WMx DB')\n",
    "        elif 'om' in process:\n",
    "            dataframe_db.append('OMx DB')\n",
    "        # print(es_host)\n",
    "        \n",
    "        ''' instance '''\n",
    "        es_obj_s_client = get_es_instance(os.getenv(es_host))\n",
    "        dataframe_es_client.append(es_obj_s_client)\n",
    "    \n",
    "        ''' return index_name and query '''\n",
    "        index_name, query = query_handler(process, process_script_query_dict)\n",
    "        # print(json.dumps(query, indent=2))\n",
    "\n",
    "        ''' Delay time for ES '''\n",
    "        StartTime = datetime.datetime.now()\n",
    "        response = es_obj_s_client.search(index=index_name, body=query)\n",
    "        # print(json.dumps(response, indent=2))\n",
    "        EndTime = datetime.datetime.now()\n",
    "\n",
    "        ''' Delay time for searching'''\n",
    "        Delay_Time = str((EndTime - StartTime).seconds) + '.' + str((EndTime - StartTime).microseconds).zfill(6)[:2]\n",
    "        dataframe_delay_time.append(float(Delay_Time))\n",
    "        \n",
    "        duplicates_list = response['aggregations']['duplicates']['buckets']\n",
    "        \n",
    "        # print(f\"total duplicates : {json.dumps(len(duplicates_list), indent=2)}\")\n",
    "        # print(json.dumps(duplicates_list, indent=2))\n",
    "        # lookup = json_value_to_transform_trim(response['aggregations'])\n",
    "        # print(f\"lookup - type(lookup) : {type(lookup)}, lookup : {lookup}\")\n",
    "    \n",
    "        ''' total docs with len(buckets) + \"sum_other_doc_count\" '''\n",
    "        '''\n",
    "        if len(duplicates_list) > 0:\n",
    "            AGGS_TOTAL_DOCS = len(duplicates_list) + int(response['aggregations']['duplicates']['sum_other_doc_count'])\n",
    "        else:\n",
    "            AGGS_TOTAL_DOCS = len(duplicates_list)\n",
    "        '''\n",
    "        AGGS_TOTAL_DOCS = len(duplicates_list)\n",
    "        # print(env, process, len(duplicates_list), int(response['aggregations']['duplicates']['sum_other_doc_count']))\n",
    "        # print(env, process, AGGS_TOTAL_DOCS, query, response)\n",
    "        # es_host_duplicates.update({es_host : AGGS_TOTAL_DOCS})\n",
    "        dataframe_value.append(AGGS_TOTAL_DOCS)\n",
    "        # dataframe_query_dsl.append(query)\n",
    "    \n",
    "        ''' Call query with inner hits for extracting _ids '''\n",
    "        lookup_query = {\n",
    "          \"size\": 10,\n",
    "          \"query\": {\n",
    "            \"bool\": {\n",
    "              \"must\": [\n",
    "                {\n",
    "                  \"terms\": {\n",
    "                    \"_id\": [\n",
    "                      \"sample\"\n",
    "                    ]\n",
    "                  }\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          }, \n",
    "          \"aggs\": {\n",
    "            \"duplicates\": {\n",
    "              \"terms\": {\n",
    "                \"script\": {\n",
    "                  \"source\": process_script_query_dict,\n",
    "                  # \"source\": \"doc['TASKID'].value\",\n",
    "                  \"lang\": \"painless\",\n",
    "                  \"params\": {\n",
    "                    \"param\": \",\"\n",
    "                  }\n",
    "                },\n",
    "                \"min_doc_count\": 2,\n",
    "                \"size\": 10000\n",
    "              },\n",
    "              \"aggs\": {\n",
    "                \"first-report\": {\n",
    "                  \"top_hits\": {\n",
    "                    \"_source\": [\"_id\"], \n",
    "                    \"size\": 10\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        \n",
    "        if AGGS_TOTAL_DOCS > 0:\n",
    "            index_name, query = query_handler(process, process_script_query_dict, basic_query=False)\n",
    "            # print(query)\n",
    "            '''\n",
    "            response = es_obj_s_client.search(index=index_name, body=query)\n",
    "            _ids_list = []\n",
    "            for each_bucket in response['aggregations']['duplicates']['buckets']:\n",
    "                for row_dict in each_bucket['first-report']['hits']['hits']:\n",
    "                    # print(row_dict.get('_id'))\n",
    "                    _ids_list.append(row_dict.get('_id'))\n",
    "            # print('\",\"'.join(_ids_list))\n",
    "            lookup_query['query']['bool']['must'][0]['terms']['_id'] = ['\",\"'.join(_ids_list)]\n",
    "            lookup_query = str(lookup_query).replace(\"'\", '\"')\n",
    "            lookup_query = str(lookup_query).replace('doc[\"', \"doc['\")\n",
    "            lookup_query = str(lookup_query).replace('\"].value', \"'].value\")\n",
    "            dataframe_ids.append(lookup_query)\n",
    "            '''\n",
    "            # dataframe_ids.append('\",\"'.join(_ids_list))\n",
    "            dataframe_ids.append(query)\n",
    "            query_env_dict[str(env).upper()].update({process : query})\n",
    "        else:\n",
    "            dataframe_ids.append(\"\")\n",
    "                \n",
    "        # ''' save raw_data '''\n",
    "        # raw_env_dict[env].update({\n",
    "        #     process : duplicates_list\n",
    "        # })\n",
    "    \n",
    "        if str(env).upper() not in raw_env_dict.keys():\n",
    "            raw_env_dict.update({str(env).upper() : len(duplicates_list)})\n",
    "        else:\n",
    "            raw_env_dict.update({str(env).upper() : int(raw_env_dict[str(env).upper()]) + len(duplicates_list)})\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(process, e)\n",
    "        dataframe_delay_time.append(float(-1))\n",
    "        dataframe_value.append(-1)\n",
    "        dataframe_ids.append(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e55348d-2e06-4854-bb2f-24a8fa550f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c080b15e71d46b6958539803a585272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ebcbfa0af74f4a9a9b5eb61dd0466b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# env_list = ['dev']\n",
    "# env_list = ['dev', 'dev']\n",
    "env_list = ['qa1','qa2']\n",
    "# env_list = ['qa1','qa2','qa4','qa5','qa6','qa9','qa11','qa13','qa14','qa15','qa16','qa17','qa18','qa20','qa25']\n",
    "# env_list = ['prod1','prod2','prod3','prod4','prod6','prod7','prod8','prod9','prod10','prod12','prod13','prod14','prod16','prod17','prod18','prod19','prod20']\n",
    "# env_list = ['prod1']\n",
    "# env_list = ['prod1','prod2']\n",
    "# env_list = ['prod1','prod2','prod3','prod4','prod6','prod7']\n",
    "# env_list = ['prod8','prod9','prod10','prod12','prod13','prod14']\n",
    "# env_list = ['prod16','prod17','prod18','prod19','prod20']\n",
    "dataframe_dict = {}\n",
    "raw_env_dict = {}\n",
    "# raw_env_dict = defaultdict(dict)\n",
    "dataframe_column, dataframe_process, dataframe_es_client, dataframe_value, dataframe_db, dataframe_query_dsl, dataframe_ids = [], [], [], [], [], [], []\n",
    "dataframe_delay_time = []\n",
    "dataframe_df_result = []\n",
    "query_env_dict = defaultdict(dict)\n",
    "\n",
    "''' Script that can check for duplicate entries for TASK documents, it would be great if we could do this for all WMx/OMx ES indices.'''\n",
    "''' Modify your script to check for any process name by the below field(s) to see if any duplicate data exists. '''\n",
    "\n",
    "''' Create query dsl with unique fields '''\n",
    "''' take long time to extract the duplicate entries '''\n",
    "# 'wx_inv_trans' : \"doc['INVTRANSKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "\n",
    "process_script_query_dict = {\n",
    "    'wx_adjustment' : \"doc['ADJUSTMENTKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_asn' : \"doc['ASNKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_asndtl' : \"doc['ASNKEY.keyword'].value + params.param + doc['ASNLINENO'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_company' : \"doc['COMPANYKEY.keyword'].value + params.param + doc['COMPANYTYPE.keyword'].value + params.param + doc['CLIENTID.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_container_ship' : \"doc['CONTAINERKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_dock_ack' : \"doc['DOCKACKKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_inv_case' : \"doc['INVCASEID.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_inv_hold' : \"doc['INVHOLDID'].value\",\n",
    "    'wx_inv_holdtrans' : \"doc['INVHOLDTRANSID'].value\",\n",
    "    'wx_inv_hold_hist' : \"doc['INVHOLDID'].value\",\n",
    "    'wx_inv_sn' : \"doc['INVSN.keyword'].value + params.param + doc['SKU.keyword'].value + params.param + doc['CLIENTID.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_inv_sn_trans' : \"doc['INVSNTRANSID'].value\",\n",
    "    # 'wx_inv_trans' : \"doc['INVTRANSKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_loc' : \"doc['LOC.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_mbol' : \"doc['MBOLKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_nci' : \"doc['NCIKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_order' : \"doc['ORDERKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_orderdtl' : \"doc['ORDERKEY.keyword'].value + params.param + doc['ORDERLINENO'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_order_casemnf' : \"doc['CASEID.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_po' : \"doc['POKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_podtl' : \"doc['POKEY.keyword'].value + params.param + doc['POLINENO'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_receipt' : \"doc['RECEIPTKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_sku' : \"doc['SKU.keyword'].value + params.param + doc['CLIENTID.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_task' : \"doc['TASKID'].value\",\n",
    "    'wx_wave' : \"doc['WAVEKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'om_appointment' : \"doc['CRN'].value\",\n",
    "    'om_asn' : \"doc['CRN'].value\",\n",
    "    'om_controlorder' : \"doc['CRN'].value\",\n",
    "    # 'om_inventorytransaction' : \"doc['SITE_ID.keyword'].value + params.param + doc['INVTRANSKEY.keyword'].value\",\n",
    "    'om_inv_balance' : \"doc['CLIENT_ID.keyword'].value + params.param + doc['SITEID.keyword'].value + params.param + doc['SKU.keyword'].value + params.param + doc['ACCOUNT.keyword'].value\",\n",
    "    'om_mbol' : \"doc['MBOLKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'om_nci' : \"doc['CRN'].value\",\n",
    "    'om_order' : \"doc['CRN'].value\",\n",
    "    # 'om_order_original_dkr' : \"doc['CRN'].value\",\n",
    "    'om_organization' : \"doc['ORGANIZATION_ID.keyword'].value\",\n",
    "    # 'om_org_client' : \"doc['ORGANIZATION_ID.keyword'].value\",\n",
    "    'om_po' : \"doc['CRN'].value\",\n",
    "    'om_receipt' : \"doc['RECEIPTKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'om_sku' : \"doc['CRN'].value\",\n",
    "    'om_whorder' : \"doc['CRN'].value\",\n",
    "}\n",
    "\n",
    "# print('Progressing..')\n",
    "pbar = tqdm(env_list)\n",
    "# for env in env_list:\n",
    "i = 0\n",
    "for env in pbar:\n",
    "    # print(f\"Progressing for {env} ..\")\n",
    "    pbar.set_description(f\"Processing {env}\")\n",
    "    # for each_index in process_script_query_dict.keys():\n",
    "    ''' Is there any option, how to show only current j progress bar during the run? You can use leave param when create progress bar.  '''\n",
    "    pbar_process = tqdm(list(process_script_query_dict.keys()), leave=bool(i == len(env_list)-1))\n",
    "    for each_index in pbar_process:\n",
    "        # print(f\"Progressing for {each_index} ..\")\n",
    "        pbar_process.set_description(f\"Processing {env} : {each_index}\")\n",
    "        check_duplicates_tasks(env, process_script_query_dict.get(each_index), each_index)\n",
    "        time.sleep(2)\n",
    "    i += 1\n",
    "# print('\\n Duplicate records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a7025bb-715c-4331-a009-1fa959af351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' update dict for dataframe '''\n",
    "dataframe_dict.update({\"ENV\" : dataframe_column})\n",
    "dataframe_dict.update({\"ES URL\" : dataframe_es_client})\n",
    "dataframe_dict.update({\"DB\" : dataframe_db})\n",
    "dataframe_dict.update({\"Query_Time\" : dataframe_delay_time})\n",
    "dataframe_dict.update({\"Process_Name\" : dataframe_process})\n",
    "dataframe_dict.update({\"Duplicates_Count\" : dataframe_value})\n",
    "dataframe_dict.update({\"Query DSL for lookup Ids\" : dataframe_ids})\n",
    "# dataframe_dict.update({'Query DSL' : dataframe_query_dsl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e144240f-c1d9-4b81-93b2-11564da159cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataframe_dict))\n",
    "# print(dataframe_dict)\n",
    "# print(json.dumps(dataframe_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba7c9f54-688c-4681-9770-d3c17ab2e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe_dict)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "690614a3-82e6-4502-8e79-d9eaff9b9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' print out raw data '''\n",
    "# print(json.dumps(raw_env_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c0aea2a-277e-4a15-9ee4-bb504ca90f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export csv files successfully..\n"
     ]
    }
   ],
   "source": [
    "# writing to Excel\n",
    "df.to_csv(\"duplicate.csv\")\n",
    "print(\"Export csv files successfully..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d51f27c-3714-4443-a0b4-409fb7462de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' filter if any duplicate data exist in df dataframe '''\n",
    "# display(df.filter(items=['ES_Cluster', 'Process_Name'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e40c4e23-514b-412f-b498-233444c56575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Check for any process name by the below field(s) to see if any duplicate data exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENV</th>\n",
       "      <th>Duplicate_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QA1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QA2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENV  Duplicate_Count\n",
       "0  QA1                6\n",
       "1  QA2                0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' env list if any duplicate data exists '''\n",
    "# print(raw_env_dict)\n",
    "print('# Check for any process name by the below field(s) to see if any duplicate data exists')\n",
    "df_dict_result = {}\n",
    "df_dict_result.update({\"ENV\" : list(raw_env_dict.keys())})\n",
    "df_dict_result.update({\"Duplicate_Count\" : list(raw_env_dict.values())})\n",
    "df_result = pd.DataFrame(df_dict_result)\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d78b6b06-5616-4f15-9f12-299116907564",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' check if any duplicate data exist in df dataframe '''\n",
    "df = df[(df['Duplicates_Count'] > 0) | (df['Query_Time'] > 1)]\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6cdb7ee-3afa-4752-ba0d-e895615b2e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"QA1\": {\n",
      "    \"wx_sku\": {\n",
      "      \"size\": 0,\n",
      "      \"aggs\": {\n",
      "        \"duplicates\": {\n",
      "          \"terms\": {\n",
      "            \"script\": {\n",
      "              \"source\": \"doc['SKU.keyword'].value + params.param + doc['CLIENTID.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
      "              \"lang\": \"painless\",\n",
      "              \"params\": {\n",
      "                \"param\": \",\"\n",
      "              }\n",
      "            },\n",
      "            \"min_doc_count\": 2,\n",
      "            \"size\": 10000\n",
      "          },\n",
      "          \"aggs\": {\n",
      "            \"first-report\": {\n",
      "              \"top_hits\": {\n",
      "                \"_source\": [\n",
      "                  \"_id\"\n",
      "                ],\n",
      "                \"size\": 10\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    \"om_sku\": {\n",
      "      \"size\": 0,\n",
      "      \"aggs\": {\n",
      "        \"duplicates\": {\n",
      "          \"terms\": {\n",
      "            \"script\": {\n",
      "              \"source\": \"doc['CRN'].value\",\n",
      "              \"lang\": \"painless\",\n",
      "              \"params\": {\n",
      "                \"param\": \",\"\n",
      "              }\n",
      "            },\n",
      "            \"min_doc_count\": 2,\n",
      "            \"size\": 10000\n",
      "          },\n",
      "          \"aggs\": {\n",
      "            \"first-report\": {\n",
      "              \"top_hits\": {\n",
      "                \"_source\": [\n",
      "                  \"_id\"\n",
      "                ],\n",
      "                \"size\": 10\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "''' print out query_dsl if any duplicate data exist on process name'''\n",
    "print(json.dumps(query_env_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f48ed7c-5da5-48b9-804e-36e1c2fb37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ******* Test code ********* '''\n",
    "def test_pytest_func():\n",
    "    assert 42 == 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be14259b-496d-43c3-9c08-4752696c653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_query_validate():\n",
    "    ''' Validate the basic query '''\n",
    "    index_name, query = query_handler('test', \"doc['TASKID'].value\")\n",
    "    # index_name, query = query_handler('test', [\"ADJUSTMENTKEY.keyword\", \"SITEID.keyword\"])\n",
    "    # print(query)\n",
    "    assert index_name == 'test'\n",
    "    assert type(query) == dict\n",
    "    '''\n",
    "    assert query == {\n",
    "      \"size\": 0,\n",
    "      \"aggs\": {\n",
    "        \"duplicates\": {\n",
    "          \"terms\": {\n",
    "            \"field\": \"ADJUSTMENTKEY.keyword\",\n",
    "            \"min_doc_count\": 2\n",
    "          },\n",
    "          \"aggs\": {\n",
    "            \"sub\": {\n",
    "              \"terms\": {\n",
    "                \"field\": \"SITEID.keyword\",\n",
    "                \"min_doc_count\": 2,\n",
    "                \"size\": 10000\n",
    "              }\n",
    "            },\n",
    "            \"min_bucket_selector\": {\n",
    "              \"bucket_selector\": {\n",
    "                \"buckets_path\": {\n",
    "                  \"count\": \"sub._bucket_count\"\n",
    "                },\n",
    "                \"script\": {\n",
    "                  \"source\": \"params.count > 0\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    assert query == {\n",
    "      \"size\": 0,\n",
    "      \"aggs\": {\n",
    "        \"duplicates\": {\n",
    "          \"terms\": {\n",
    "            \"script\": {\n",
    "              \"source\": \"doc['TASKID'].value\",\n",
    "              # \"source\": \"doc['ORDERKEY.keyword'].value + params.param + doc['ORDERLINENO'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "              \"lang\": \"painless\",\n",
    "              \"params\": {\n",
    "                \"param\": \",\"\n",
    "              }\n",
    "            },\n",
    "            \"min_doc_count\": 2,\n",
    "            \"size\": 10000\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    ''' Validate the extract ids query '''\n",
    "    index_name, query = query_handler('test', \"doc['TASKID'].value\", basic_query=False)\n",
    "    assert query == {\n",
    "      \"size\": 0,\n",
    "      \"aggs\": {\n",
    "        \"duplicates\": {\n",
    "          \"terms\": {\n",
    "            \"script\": {\n",
    "              \"source\": \"doc['TASKID'].value\",\n",
    "              # \"source\": \"doc['ORDERKEY.keyword'].value + params.param + doc['ORDERLINENO'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "              \"lang\": \"painless\",\n",
    "              \"params\": {\n",
    "                \"param\": \",\"\n",
    "              }\n",
    "            },\n",
    "            \"min_doc_count\": 2,\n",
    "            \"size\": 10000\n",
    "          },\n",
    "          \"aggs\": {\n",
    "              \"first-report\": {\n",
    "                  \"top_hits\": {\n",
    "                    \"_source\": [\"_id\"], \n",
    "                    \"size\": 10\n",
    "                  }\n",
    "               }\n",
    "           }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e87d4f30-bba1-4d9a-8390-f6f173bc8457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_script_query_to_query_list():\n",
    "    ''' Validate script query when transforming to multiple fields '''\n",
    "    \n",
    "    ''' Case #1 '''\n",
    "    script_query = script_query_to_query_list(\"doc['RECEIPTKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\")\n",
    "    assert script_query == ['RECEIPTKEY.keyword', 'SITEID.keyword']\n",
    "\n",
    "    ''' Case #2 '''\n",
    "    script_query = script_query_to_query_list(\"doc['COMPANYKEY.keyword'].value + params.param + doc['COMPANYTYPE.keyword'].value + params.param + doc['CLIENTID.keyword'].value + params.param + doc['SITEID.keyword'].value\")\n",
    "    assert script_query == ['COMPANYKEY.keyword', 'COMPANYTYPE.keyword', 'CLIENTID.keyword', 'SITEID.keyword']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a23178f5-c36b-473a-b850-e44abbc74608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.9.0, pytest-8.3.4, pluggy-1.5.0 -- /home/biadmin/monitoring/jupyter_notebook/.venv/bin/python3.9\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/biadmin/monitoring/jupyter_notebook\n",
      "plugins: anyio-4.5.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "t_e856622c151d43c7aa5798ce80ebd5dc.py::test_pytest_func \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 33%]\u001b[0m\n",
      "t_e856622c151d43c7aa5798ce80ebd5dc.py::test_query_validate \u001b[32mPASSED\u001b[0m\u001b[32m                            [ 66%]\u001b[0m\n",
      "t_e856622c151d43c7aa5798ce80ebd5dc.py::test_script_query_to_query_list \u001b[32mPASSED\u001b[0m\u001b[32m                [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Execute the tests using ipytest.run(). You can pass command-line arguments to control test behavior: '''\n",
    "''' The pytest framework makes it easy to write small, readable tests, and can scale to support complex functional testing for applications and libraries. '''\n",
    "ipytest.run('-vv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867d006-a91b-4a8d-9a85-7220fe95d6d3",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
