{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a176f610-cf34-41d3-902d-cac996cb5878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the records if index has duplicate recores\n"
     ]
    }
   ],
   "source": [
    "''' Jupyter Notebook is an incredibly powerful tool for interactively developing and presenting data science projects.  '''\n",
    "''' It allows you to run code, display the results, and add explanations, formulas, and charts all in one place.  '''\n",
    "\n",
    "''' Script that can check for duplicate entries for TASK documents, it would be great if we could do this for all WMx/OMx ES indices.'''\n",
    "''' Modify your script to check for any process name by the below field(s) to see if any duplicate data exists. '''\n",
    "print(f\"Check the records if index has duplicate recores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "bfa7a8d7-7409-4020-a4e2-ec8b907c6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y elasticsearch\n",
    "#!pip install elasticsearch==7.13.0\n",
    "''' To begin, you need to install pytest and ipytest, a tool designed to run pytest tests directly in Jupyter. Execute the following in a Jupyter cell '''\n",
    "# !pip install pytest ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6a58b73c-bd80-4906-b7df-57e3c87c6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "01c8a2d0-6fea-47f6-aa47-7272e41717ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: elasticsearch\n",
      "Version: 7.13.0\n",
      "Summary: Python client for Elasticsearch\n",
      "Home-page: https://github.com/elastic/elasticsearch-py\n",
      "Author: Honza Kr√°l, Nick Lang\n",
      "Author-email: honza.kral@gmail.com, nick@nicklang.com\n",
      "License: Apache-2.0\n",
      "Location: /home/biadmin/monitoring/jupyter_notebook/.venv/lib/python3.9/site-packages\n",
      "Requires: certifi, urllib3\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8f63ef6e-8aec-4897-a230-d2ff40ea3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "# import pytest\n",
    "import ipytest\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "86f106ca-c5b2-4907-aade-b7f6b81fefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' To begin, you need to install pytest and ipytest, a tool designed to run pytest tests directly in Jupyter. Execute the following in a Jupyter cell '''\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "df8cea7f-c5ae-4cab-ae5d-9d7a272a35e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' pip install python-dotenv'''\n",
    "load_dotenv() # will search for .env file in local folder and load variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3fcd93bf-191a-4bff-876f-010802bf629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers():\n",
    "    ''' Elasticsearch Header '''\n",
    "    return {\n",
    "            'Content-type': 'application/json', \n",
    "            'Authorization' : '{}'.format(os.getenv('BASIC_AUTH')),\n",
    "            'Connection': 'close'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2d9a80da-4712-41ef-b5db-a1aea3e96f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_es_instance(host):\n",
    "    es_client = Elasticsearch(hosts=\"http://{}\".format(host), headers=get_headers(), timeout=5,  verify_certs=False)\n",
    "    return es_client\n",
    "\n",
    "# es_obj_s_client = get_es_instance(\"localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a0238edd-55c4-4f4d-8ac6-43233b143bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp = es_obj_s_client.cluster.health()\n",
    "# print(json.dumps(resp, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "62db2a74-e4c0-4f5b-8a79-e44cea9f050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_host_duplicates = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a986a0be-0f8c-446e-8b07-f0c43871a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_value_to_transform_trim(raw_json):\n",
    "    ''' update value in the form of json format'''\n",
    "    # print(f\"raw_json : {raw_json}\")\n",
    "    def get_recursive_nested_all(d):\n",
    "        # print(f\"get_recursive_nested_all : {d}\")\n",
    "        if isinstance(d, list):\n",
    "            for i in d:\n",
    "                get_recursive_nested_all(i)\n",
    "        elif isinstance(d, dict):\n",
    "            for k, v in d.items():\n",
    "                if not isinstance(v, (list, dict)):\n",
    "                    # print(\"%%%%\", k, v)\n",
    "                    d[k] = v\n",
    "                else:\n",
    "                    # print(\"####\", k, v)\n",
    "                    get_recursive_nested_all(v)\n",
    "        return d\n",
    "\n",
    "    return get_recursive_nested_all(raw_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3464c3fc-69d0-43c3-9f54-ab0e77e238ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_handler(process, script_query):\n",
    "    ''' check dsl for any duplicates '''\n",
    "    '''https://stackoverflow.com/questions/53076349/script-writing-to-get-distinct-value-from-elasticsearch '''\n",
    "    ''' https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-script '''\n",
    "    ''' So you can see in the results as how key are constructed (keys are unique). '''\n",
    "    '''\n",
    "    {\n",
    "      \"size\": 0,\n",
    "      \"aggs\": {\n",
    "        \"duplicates\": {\n",
    "          \"terms\": {\n",
    "            \"script\": {\n",
    "              #\"source\": \"doc['TASKID'].value\",\n",
    "              \"source\": \"doc['ORDERKEY.keyword'].value + params.param + doc['ORDERLINENO'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "              \"lang\": \"painless\",\n",
    "              \"params\": {\n",
    "                \"param\": \",\"\n",
    "              }\n",
    "            },\n",
    "            \"min_doc_count\": 2,\n",
    "            \"size\": 10000\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    query = {\n",
    "          \"size\": 0,\n",
    "          \"aggs\": {\n",
    "            \"duplicates\": {\n",
    "              \"terms\": {\n",
    "                \"script\": {\n",
    "                  # \"source\": \"doc['TASKID'].value\",\n",
    "                  \"source\": script_query,\n",
    "                  \"lang\": \"painless\",\n",
    "                  \"params\": {\n",
    "                    \"param\": \",\"\n",
    "                  }\n",
    "                },\n",
    "                \"min_doc_count\": 2,\n",
    "                \"size\" : 10000\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "\n",
    "    extract_ids_query = {\n",
    "          \"size\": 0,\n",
    "          \"aggs\": {\n",
    "            \"duplicates\": {\n",
    "              \"terms\": {\n",
    "                \"script\": {\n",
    "                  # \"source\": \"doc['TASKID'].value\",\n",
    "                  \"source\": script_query,\n",
    "                  \"lang\": \"painless\",\n",
    "                  \"params\": {\n",
    "                    \"param\": \",\"\n",
    "                  }\n",
    "                },\n",
    "                \"min_doc_count\": 2,\n",
    "                \"size\" : 10000\n",
    "              },\n",
    "              \"aggs\": {\n",
    "                \"first-report\": {\n",
    "                  \"top_hits\": {\n",
    "                    \"_source\": [\"_id\"], \n",
    "                    \"size\": 10\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    # if process == 'sample':\n",
    "        # query = {\n",
    "        #   \"size\": 0,\n",
    "        #   \"aggs\": {\n",
    "        #     \"duplicates\": {\n",
    "        #       \"terms\": {\n",
    "        #             \"field\": \"TASKID\",\n",
    "        #             \"min_doc_count\": 2,\n",
    "        #             \"size\" : 10000  \n",
    "        #         }\n",
    "        #      }\n",
    "        #   }\n",
    "        # }\n",
    "\n",
    "    # print(json.dumps(query, indent=2))\n",
    "    # exit(1)\n",
    "    \n",
    "    return process, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "80a63282-6377-4ab8-8a68-4bc89251e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_tasks(env, process_script_query_dict, process):\n",
    "    ''' check duplicates'''\n",
    "    \n",
    "    # print('\\n', env)\n",
    "    # print(os.getenv(\"{}_ES_HOST\".format(str(env).upper())))\n",
    "    es_host = \"{}_ES_HOST\".format(str(env).upper())\n",
    "    dataframe_column.append(es_host)\n",
    "    dataframe_process.append(process)\n",
    "\n",
    "    if 'wx' in process:\n",
    "        dataframe_db.append('WMx DB')\n",
    "    elif 'om' in process:\n",
    "        dataframe_db.append('OMx DB')\n",
    "    # print(es_host)\n",
    "    \n",
    "    ''' instance '''\n",
    "    es_obj_s_client = get_es_instance(os.getenv(es_host))\n",
    "    dataframe_es_client.append(es_obj_s_client)\n",
    "\n",
    "    ''' return index_name and query '''\n",
    "    index_name, query = query_handler(process, process_script_query_dict)\n",
    "    \n",
    "    response = es_obj_s_client.search(index=index_name, body=query)\n",
    "    # print(json.dumps(response, indent=2))\n",
    "    \n",
    "    duplicates_list = response['aggregations']['duplicates']['buckets']\n",
    "    \n",
    "    # print(f\"total duplicates : {json.dumps(len(duplicates_list), indent=2)}\")\n",
    "    # print(json.dumps(duplicates_list, indent=2))\n",
    "    # lookup = json_value_to_transform_trim(response['aggregations'])\n",
    "    # print(f\"lookup - type(lookup) : {type(lookup)}, lookup : {lookup}\")\n",
    "\n",
    "    ''' total docs with len(buckets) + \"sum_other_doc_count\" '''\n",
    "    '''\n",
    "    if len(duplicates_list) > 0:\n",
    "        AGGS_TOTAL_DOCS = len(duplicates_list) + int(response['aggregations']['duplicates']['sum_other_doc_count'])\n",
    "    else:\n",
    "        AGGS_TOTAL_DOCS = len(duplicates_list)\n",
    "    '''\n",
    "    AGGS_TOTAL_DOCS = len(duplicates_list)\n",
    "    # print(env, process, len(duplicates_list), int(response['aggregations']['duplicates']['sum_other_doc_count']))\n",
    "    # print(env, process, AGGS_TOTAL_DOCS, query, response)\n",
    "    es_host_duplicates.update({es_host : AGGS_TOTAL_DOCS})\n",
    "    dataframe_value.append(AGGS_TOTAL_DOCS)\n",
    "    # dataframe_query_dsl.append(query)\n",
    "\n",
    "    ''' save raw_data '''\n",
    "    raw_env_dict[env].update({\n",
    "        process : duplicates_list\n",
    "    })\n",
    "\n",
    "    # return dataframe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3e55348d-2e06-4854-bb2f-24a8fa550f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressing for prod1 ..\n",
      "Progressing for prod2 ..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# env_list = ['prod1','prod2','prod3','prod4','prod6','prod7','prod8','prod9','prod10','prod12','prod13','prod14','prod16','prod17','prod18','prod19','prod20']\n",
    "# env_list = ['dev']\n",
    "# env_list = ['prod1']\n",
    "env_list = ['prod1','prod2']\n",
    "dataframe_dict = {}\n",
    "raw_env_dict = defaultdict(dict)\n",
    "dataframe_column, dataframe_process, dataframe_es_client, dataframe_value, dataframe_db, dataframe_query_dsl = [], [], [], [], [], []\n",
    "\n",
    "''' Script that can check for duplicate entries for TASK documents, it would be great if we could do this for all WMx/OMx ES indices.'''\n",
    "''' Modify your script to check for any process name by the below field(s) to see if any duplicate data exists. '''\n",
    "\n",
    "''' Create query dsl with unique fields '''\n",
    "process_script_query_dict = {\n",
    "    'wx_adjustment' : \"doc['ADJUSTMENTKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_asn' : \"doc['ASNKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_asndtl' : \"doc['ASNKEY.keyword'].value + params.param + doc['ASNLINENO'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_task' : \"doc['TASKID'].value\",\n",
    "    'wx_orderdtl' : \"doc['ORDERKEY.keyword'].value + params.param + doc['ORDERLINENO'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "    'wx_inv_hold' : \"doc['INVHOLDID'].value\",\n",
    "    'wx_inv_holdtrans' : \"doc['INVHOLDTRANSID'].value\",\n",
    "    'om_appointment' : \"doc['CRN'].value\"\n",
    "}\n",
    "\n",
    "for env in env_list:\n",
    "    print(f\"Progressing for {env} ..\")\n",
    "    for each_index in process_script_query_dict.keys():\n",
    "        check_duplicates_tasks(env, process_script_query_dict.get(each_index), each_index)\n",
    "# print('\\n Duplicate records')\n",
    "# print(json.dumps(dataframe_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5a7025bb-715c-4331-a009-1fa959af351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' update dict for dataframe '''\n",
    "dataframe_dict.update({'ES_Cluster' : dataframe_column})\n",
    "dataframe_dict.update({'ES URL' : dataframe_es_client})\n",
    "dataframe_dict.update({'DB' : dataframe_db})\n",
    "dataframe_dict.update({'Process_Name' : dataframe_process})\n",
    "dataframe_dict.update({'Duplicates_Count' : dataframe_value})\n",
    "# dataframe_dict.update({'Query DSL' : dataframe_query_dsl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ba7c9f54-688c-4681-9770-d3c17ab2e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe_dict)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "690614a3-82e6-4502-8e79-d9eaff9b9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' print out raw data '''\n",
    "# print(json.dumps(raw_env_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "3c0aea2a-277e-4a15-9ee4-bb504ca90f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to Excel\n",
    "# df.to_csv(\"duplicate.csv\")\n",
    "# print(\"Export csv files successfully..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0d51f27c-3714-4443-a0b4-409fb7462de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' filter if any duplicate data exist in df dataframe '''\n",
    "# display(df.filter(items=['ES_Cluster', 'Process_Name'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d78b6b06-5616-4f15-9f12-299116907564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ES_Cluster</th>\n",
       "      <th>ES URL</th>\n",
       "      <th>DB</th>\n",
       "      <th>Process_Name</th>\n",
       "      <th>Duplicates_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ES_Cluster, ES URL, DB, Process_Name, Duplicates_Count]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' check if any duplicate data exist in df dataframe '''\n",
    "df = df[df['Duplicates_Count'] > 0]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6f48ed7c-5da5-48b9-804e-36e1c2fb37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pytest_func():\n",
    "    assert 42 == 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "be14259b-496d-43c3-9c08-4752696c653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_query_validate():\n",
    "    ''' Validate the query '''\n",
    "    index_name, query = query_handler('test', \"doc['TASKID'].value\")\n",
    "    # print(query)\n",
    "    assert index_name == 'test'\n",
    "    assert type(query) == dict\n",
    "    assert query == {\n",
    "      \"size\": 0,\n",
    "      \"aggs\": {\n",
    "        \"duplicates\": {\n",
    "          \"terms\": {\n",
    "            \"script\": {\n",
    "              \"source\": \"doc['TASKID'].value\",\n",
    "              # \"source\": \"doc['ORDERKEY.keyword'].value + params.param + doc['ORDERLINENO'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "              \"lang\": \"painless\",\n",
    "              \"params\": {\n",
    "                \"param\": \",\"\n",
    "              }\n",
    "            },\n",
    "            \"min_doc_count\": 2,\n",
    "            \"size\": 10000\n",
    "          }        }\n",
    "      }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a23178f5-c36b-473a-b850-e44abbc74608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.9.0, pytest-8.3.4, pluggy-1.5.0 -- /home/biadmin/monitoring/jupyter_notebook/.venv/bin/python3.9\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/biadmin/monitoring/jupyter_notebook\n",
      "plugins: anyio-4.5.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "t_4ad4cffb718845b3915bc958fa7a6001.py::test_pytest_func \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 50%]\u001b[0m\n",
      "t_4ad4cffb718845b3915bc958fa7a6001.py::test_query_validate \u001b[32mPASSED\u001b[0m\u001b[32m                            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Execute the tests using ipytest.run(). You can pass command-line arguments to control test behavior: '''\n",
    "''' The pytest framework makes it easy to write small, readable tests, and can scale to support complex functional testing for applications and libraries. '''\n",
    "ipytest.run('-vv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
