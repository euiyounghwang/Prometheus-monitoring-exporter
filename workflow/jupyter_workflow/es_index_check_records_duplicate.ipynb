{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a176f610-cf34-41d3-902d-cac996cb5878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the records if index has duplicate recores\n"
     ]
    }
   ],
   "source": [
    "''' Jupyter Notebook is an incredibly powerful tool for interactively developing and presenting data science projects.  '''\n",
    "''' It allows you to run code, display the results, and add explanations, formulas, and charts all in one place.  '''\n",
    "\n",
    "''' Script that can check for duplicate entries for TASK documents, it would be great if we could do this for all WMx/OMx ES indices.'''\n",
    "''' Modify your script to check for any process name by the below field(s) to see if any duplicate data exists. '''\n",
    "print(f\"Check the records if index has duplicate recores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfa7a8d7-7409-4020-a4e2-ec8b907c6670",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (330285866.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[24], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    ''' ''' To begin, you need to install pytest and ipytest, a tool designed to run pytest tests directly in Jupyter. Execute the following in a Jupyter cell '''\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall -y elasticsearch\n",
    "#!pip install elasticsearch==7.13.0\n",
    "''' To begin, you need to install pytest and ipytest, a tool designed to run pytest tests directly in Jupyter. Execute the following in a Jupyter cell '''\n",
    "# !pip install pytest ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a58b73c-bd80-4906-b7df-57e3c87c6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c8a2d0-6fea-47f6-aa47-7272e41717ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63ef6e-8aec-4897-a230-d2ff40ea3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "# import pytest\n",
    "import ipytest\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f106ca-c5b2-4907-aade-b7f6b81fefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' To begin, you need to install pytest and ipytest, a tool designed to run pytest tests directly in Jupyter. Execute the following in a Jupyter cell '''\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8cea7f-c5ae-4cab-ae5d-9d7a272a35e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' pip install python-dotenv'''\n",
    "load_dotenv() # will search for .env file in local folder and load variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd93bf-191a-4bff-876f-010802bf629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers():\n",
    "    ''' Elasticsearch Header '''\n",
    "    return {\n",
    "            'Content-type': 'application/json', \n",
    "            'Authorization' : '{}'.format(os.getenv('BASIC_AUTH')),\n",
    "            'Connection': 'close'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a80da-4712-41ef-b5db-a1aea3e96f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_es_instance(host):\n",
    "    es_client = Elasticsearch(hosts=\"http://{}\".format(host), headers=get_headers(), timeout=5,  verify_certs=False)\n",
    "    return es_client\n",
    "\n",
    "# es_obj_s_client = get_es_instance(\"localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0238edd-55c4-4f4d-8ac6-43233b143bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp = es_obj_s_client.cluster.health()\n",
    "# print(json.dumps(resp, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62db2a74-e4c0-4f5b-8a79-e44cea9f050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_host_duplicates = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986a0be-0f8c-446e-8b07-f0c43871a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_value_to_transform_trim(raw_json):\n",
    "    ''' update value in the form of json format'''\n",
    "    # print(f\"raw_json : {raw_json}\")\n",
    "    def get_recursive_nested_all(d):\n",
    "        # print(f\"get_recursive_nested_all : {d}\")\n",
    "        if isinstance(d, list):\n",
    "            for i in d:\n",
    "                get_recursive_nested_all(i)\n",
    "        elif isinstance(d, dict):\n",
    "            for k, v in d.items():\n",
    "                if not isinstance(v, (list, dict)):\n",
    "                    # print(\"%%%%\", k, v)\n",
    "                    d[k] = v\n",
    "                else:\n",
    "                    # print(\"####\", k, v)\n",
    "                    get_recursive_nested_all(v)\n",
    "        return d\n",
    "\n",
    "    return get_recursive_nested_all(raw_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464c3fc-69d0-43c3-9f54-ab0e77e238ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_handler(process):\n",
    "    ''' check dsl for any duplicates '''\n",
    "    '''https://stackoverflow.com/questions/53076349/script-writing-to-get-distinct-value-from-elasticsearch '''\n",
    "    ''' https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-script '''\n",
    "    ''' So you can see in the results as how key are constructed (keys are unique). '''\n",
    "    if process == 'sample':\n",
    "        query = {\n",
    "          \"size\": 0,\n",
    "          \"aggs\": {\n",
    "            \"duplicates\": {\n",
    "              \"terms\": {\n",
    "                    \"field\": \"TASKID\",\n",
    "                    \"min_doc_count\": 2,\n",
    "                    \"size\" : 10000  \n",
    "                }\n",
    "             }\n",
    "          }\n",
    "        }\n",
    "    elif process == 'wx_task':\n",
    "        query = {\n",
    "          \"size\": 0,\n",
    "          \"aggs\": {\n",
    "            \"duplicates\": {\n",
    "              \"terms\": {\n",
    "                \"script\": {\n",
    "                  \"source\": \"doc['TASKID'].value\",\n",
    "                  \"lang\": \"painless\",\n",
    "                  \"params\": {\n",
    "                    \"param\": \",\"\n",
    "                  }\n",
    "                },\n",
    "                \"min_doc_count\": 2,\n",
    "                \"size\" : 10000\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    elif process == 'wx_asn':\n",
    "         query = {\n",
    "          \"size\": 0,\n",
    "          \"aggs\": {\n",
    "            \"duplicates\": {\n",
    "              \"terms\": {\n",
    "                \"script\": {\n",
    "                  \"source\": \"doc['ASNKEY.keyword'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "                  \"lang\": \"painless\",\n",
    "                  \"params\": {\n",
    "                    \"param\": \",\"\n",
    "                  }\n",
    "                },\n",
    "                \"min_doc_count\": 2,\n",
    "                \"size\" : 10000\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    elif process == 'wx_orderdtl':\n",
    "        query = {\n",
    "          \"size\": 0,\n",
    "          \"aggs\": {\n",
    "            \"duplicates\": {\n",
    "              \"terms\": {\n",
    "                \"script\": {\n",
    "                  \"source\": \"doc['ORDERKEY.keyword'].value + params.param + doc['ORDERLINENO'].value + params.param + doc['SITEID.keyword'].value\",\n",
    "                  \"lang\": \"painless\",\n",
    "                  \"params\": {\n",
    "                    \"param\": \",\"\n",
    "                  }\n",
    "                },\n",
    "                \"min_doc_count\": 2,\n",
    "                \"size\" : 10000\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    elif process == 'wx_inv_hold':\n",
    "        query = {\n",
    "          \"size\": 0,\n",
    "          \"aggs\": {\n",
    "            \"duplicates\": {\n",
    "              \"terms\": {\n",
    "                \"script\": {\n",
    "                  \"source\": \"doc['INVHOLDID'].value\",\n",
    "                  \"lang\": \"painless\",\n",
    "                  \"params\": {\n",
    "                    \"param\": \",\"\n",
    "                  }\n",
    "                },\n",
    "                \"min_doc_count\": 2,\n",
    "                \"size\" : 10000\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    elif process == 'wx_inv_holdtrans':\n",
    "        query = {\n",
    "          \"size\": 0,\n",
    "          \"aggs\": {\n",
    "            \"duplicates\": {\n",
    "              \"terms\": {\n",
    "                \"script\": {\n",
    "                  \"source\": \"doc['INVHOLDTRANSID'].value\",\n",
    "                  \"lang\": \"painless\",\n",
    "                  \"params\": {\n",
    "                    \"param\": \",\"\n",
    "                  }\n",
    "                },\n",
    "                \"min_doc_count\": 2,\n",
    "                \"size\" : 10000\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    elif process == 'om_appointment':\n",
    "        query = {\n",
    "          \"size\": 0,\n",
    "          \"aggs\": {\n",
    "            \"duplicates\": {\n",
    "              \"terms\": {\n",
    "                \"script\": {\n",
    "                  \"source\": \"doc['CRN'].value\",\n",
    "                  \"lang\": \"painless\",\n",
    "                  \"params\": {\n",
    "                    \"param\": \",\"\n",
    "                  }\n",
    "                },\n",
    "                \"min_doc_count\": 2,\n",
    "                \"size\" : 10000\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    return process, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a63282-6377-4ab8-8a68-4bc89251e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_tasks(env, process):\n",
    "    ''' check duplicates'''\n",
    "    \n",
    "    # print('\\n', env)\n",
    "    # print(os.getenv(\"{}_ES_HOST\".format(str(env).upper())))\n",
    "    es_host = \"{}_ES_HOST\".format(str(env).upper())\n",
    "    dataframe_column.append(es_host)\n",
    "    dataframe_process.append(process)\n",
    "\n",
    "    if 'wx' in process:\n",
    "        dataframe_db.append('WMx DB')\n",
    "    elif 'om' in process:\n",
    "        dataframe_db.append('OMx DB')\n",
    "    # print(es_host)\n",
    "    \n",
    "    ''' instance '''\n",
    "    es_obj_s_client = get_es_instance(os.getenv(es_host))\n",
    "    dataframe_es_client.append(es_obj_s_client)\n",
    "\n",
    "    ''' return index_name and query '''\n",
    "    index_name, query = query_handler(process)\n",
    "    \n",
    "    response = es_obj_s_client.search(index=index_name, body=query)\n",
    "    # print(json.dumps(response, indent=2))\n",
    "    \n",
    "    duplicates_list = response['aggregations']['duplicates']['buckets']\n",
    "    \n",
    "    # print(f\"total duplicates : {json.dumps(len(duplicates_list), indent=2)}\")\n",
    "    # print(json.dumps(duplicates_list, indent=2))\n",
    "    # lookup = json_value_to_transform_trim(response['aggregations'])\n",
    "    # print(f\"lookup - type(lookup) : {type(lookup)}, lookup : {lookup}\")\n",
    "\n",
    "    es_host_duplicates.update({es_host : len(duplicates_list)})\n",
    "    dataframe_value.append(len(duplicates_list))\n",
    "\n",
    "    # return dataframe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55348d-2e06-4854-bb2f-24a8fa550f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_list = ['prod1','prod2','prod3','prod4','prod6','prod7','prod8','prod9','prod10','prod12','prod13','prod14','prod16','prod17','prod18','prod19','prod20']\n",
    "env_list = ['prod1','prod2']\n",
    "dataframe_dict = {}\n",
    "dataframe_column, dataframe_process, dataframe_es_client, dataframe_value, dataframe_db = [], [], [], [], []\n",
    "\n",
    "''' Script that can check for duplicate entries for TASK documents, it would be great if we could do this for all WMx/OMx ES indices.'''\n",
    "''' Modify your script to check for any process name by the below field(s) to see if any duplicate data exists. '''\n",
    "# process_name = ['wx_task', 'wx_inv_hold', 'wx_inv_holdtrans', 'om_appointment', 'wx_asn']\n",
    "process_name = ['wx_task', 'wx_asn', 'wx_orderdtl']\n",
    "for env in env_list:\n",
    "    print(f\"Progressing for {env} ..\")\n",
    "    for each_index in process_name:\n",
    "        check_duplicates_tasks(env, each_index)\n",
    "# print('\\n Duplicate records')\n",
    "# print(json.dumps(dataframe_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7025bb-715c-4331-a009-1fa959af351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' update dict for dataframe '''\n",
    "dataframe_dict.update({'ES_Cluster' : dataframe_column})\n",
    "dataframe_dict.update({'ES URL' : dataframe_es_client})\n",
    "dataframe_dict.update({'DB' : dataframe_db})\n",
    "dataframe_dict.update({'Process_Name' : dataframe_process})\n",
    "dataframe_dict.update({'Duplicates_Count' : dataframe_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c9f54-688c-4681-9770-d3c17ab2e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe_dict)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0aea2a-277e-4a15-9ee4-bb504ca90f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to Excel\n",
    "# df.to_csv(\"duplicate.csv\")\n",
    "# print(\"Export csv files successfully..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51f27c-3714-4443-a0b4-409fb7462de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' filter if any duplicate data exist in df dataframe '''\n",
    "# display(df.filter(items=['ES_Cluster', 'Process_Name'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b6b06-5616-4f15-9f12-299116907564",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' check if any duplicate data exist in df dataframe '''\n",
    "df = df[df['Duplicates_Count'] > 0]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48ed7c-5da5-48b9-804e-36e1c2fb37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pytest_func():\n",
    "    assert 42 == 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23178f5-c36b-473a-b850-e44abbc74608",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Execute the tests using ipytest.run(). You can pass command-line arguments to control test behavior: '''\n",
    "ipytest.run('-vv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
