{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310baf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Data Pipeline Status **\n"
     ]
    }
   ],
   "source": [
    "''' Jupyter Notebook is an incredibly powerful tool for interactively developing and presenting data science projects.  '''\n",
    "''' It allows you to run code, display the results, and add explanations, formulas, and charts all in one place.  '''\n",
    "''' This job is used to process data for both WMx/OMx Elastic (ES) pipeline queue’s for uploading of data into ES. '''\n",
    "# ** Prometheus Monitoring DB Test ** \n",
    "print(f\"** Data Pipeline Status **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d331abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import datetime\n",
    "import requests\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f2d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"db-api-call\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da5cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' pip install python-dotenv'''\n",
    "# load_dotenv() # will search for .env file in local folder and load variables \n",
    "# Reload the variables from your .env file, overriding existing ones\n",
    "dotenv.load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_difference(DB_NAME, audit_process_name_time):\n",
    "        ''' get time difference'''\n",
    "        audit_process_name_time = datetime.datetime.strptime(audit_process_name_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # lock.acquire()\n",
    "        ''' test has CDT time '''\n",
    "        if str(DB_NAME).upper() == 'test':\n",
    "            now_time = datetime.datetime.now() - datetime.timedelta(hours=1)\n",
    "        else:\n",
    "            now_time = datetime.datetime.now()\n",
    "\n",
    "        # print(f\"audit_process_name_time - {audit_process_name_time}, : {type(audit_process_name_time)}, now_time - {now_time} : {type(now_time)}\")\n",
    "        date_diff = now_time-audit_process_name_time\n",
    "        # date_diffs = (now_time-audit_process_name_time)\n",
    "        # date_diff = audit_process_name_time-now_time\n",
    "        # print(f\"Time Difference - {date_diff}\")\n",
    "        time_hours = date_diff.seconds / 3600\n",
    "        # print(f\"Time Difference to hours - {time_hours}\")\n",
    "        # lock.release()\n",
    "\n",
    "        if now_time < audit_process_name_time:\n",
    "            # logging.info(f\"now_time < audit_process_name_time - {round(-time_hours, 2)}\")\n",
    "            if time_hours == 0:\n",
    "                return 0\n",
    "            return round(-time_hours, 2)\n",
    "        else:\n",
    "            # logging.info(f\"now_time > audit_process_name_time - {round(time_hours, 2)}\")\n",
    "            return round(time_hours, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' call to DB interface RestAPI'''\n",
    "pd_colmuns, pd_values = [], []\n",
    "pd_dataframe_dict = {}\n",
    "def get_db_active(env, sql):\n",
    "    db_url = os.getenv(str(env).upper())\n",
    "    request_body = {\n",
    "        \"db_url\" : db_url,\n",
    "        \"sql\" : sql\n",
    "    }\n",
    "    \n",
    "    # logging.info(\"db_http_host : {}, db_url : {}, sql : {}\".format(os.getenv(\"DB_API_HOST\"), db_url, sql))\n",
    "    http_urls = \"http://{}/db/get_db_query\".format(os.getenv(\"DB_API_HOST\"))\n",
    "    resp = requests.post(url=http_urls, json=request_body, timeout=600)\n",
    "    # logging.info(f\"resp status code : {resp.status_code}\")\n",
    "\n",
    "    ''' resp body '''\n",
    "    response = resp.json()\n",
    "    # logging.info(json.dumps(resp.json(), indent=2))\n",
    "    # logging.info(type(response))\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    '''\n",
    "    for row in response['results']:\n",
    "        for k, v in row.items():\n",
    "            if k not in pd_dataframe_dict.keys():\n",
    "                pd_dataframe_dict.update({k : [str(v)]})\n",
    "            else:\n",
    "                pd_dataframe_dict.update({k : pd_dataframe_dict.get(k) + [v]})\n",
    "\n",
    "    # logging.info(json.dumps(pd_dataframe_dict, indent=2))\n",
    "    return pd_dataframe_dict\n",
    "    '''\n",
    "    # return response['results']        \n",
    "    # return response\n",
    "    ''' \n",
    "    data = {\n",
    "        'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "        'Age': [25, 30, 35, 40]\n",
    "    }\n",
    "     \n",
    "    df = pd.DataFrame(data)\n",
    "           Name  Age\n",
    "    0     Alice   25\n",
    "    1       Bob   30\n",
    "    2   Charlie   35\n",
    "    3     David   40\n",
    "    '''\n",
    "    results_records_dict = {}\n",
    "    processname, status, addts, processed_recs, dbid = [],[],[],[],[]\n",
    "    loop = 0\n",
    "    recent_data_pipeline_processed = -1\n",
    "    for each_row in response['results']:\n",
    "        if loop < 1:\n",
    "                recent_data_pipeline_processed = each_row.get(\"ADDTS\")\n",
    "        processname.append(each_row.get(\"PROCESSNAME\"))\n",
    "        status.append(each_row.get(\"STATUS\"))\n",
    "        addts.append(each_row.get(\"ADDTS\"))\n",
    "        processed_recs.append(\"{:,}\".format(each_row.get(\"COUNT(*)\")))\n",
    "        dbid.append(each_row.get(\"DBID\"))\n",
    "        loop += 1\n",
    "\n",
    "    ''' around 30 mintues '''\n",
    "    ''' Subtract the current timestamp and the max_addts to determine ''' \n",
    "    time_threshold =  0.6\n",
    "    data_pipeline_result = [''] * len(processname)\n",
    "    DATA_PIPELINE_PRCESSED_WORK = 'FAIL'\n",
    "    CHAR_SPLIT = \"_\"\n",
    "\n",
    "    if CHAR_SPLIT in env:    \n",
    "            time_gap_recent_data_pipeline_processed = get_time_difference(env.split(CHAR_SPLIT)[0], recent_data_pipeline_processed)\n",
    "            # print(f\"recent_data_pipeline_processed : {recent_data_pipeline_processed}, time_gap_recent_data_pipeline_processed : {time_gap_recent_data_pipeline_processed}\")\n",
    "            if time_gap_recent_data_pipeline_processed < time_threshold:\n",
    "                    DATA_PIPELINE_PRCESSED_WORK = 'OK'\n",
    "                    data_pipeline_result[0] = DATA_PIPELINE_PRCESSED_WORK\n",
    "            else:\n",
    "                    data_pipeline_result[0] = 'FAIL'\n",
    "                    \n",
    "    results_records_dict.update({\n",
    "            'PROCESSNAME' : processname, \n",
    "            'STATUS' : status, \n",
    "            'ADDTS' : addts, \n",
    "            'COUNT(*)' : processed_recs, \n",
    "            'DBID' : dbid,\n",
    "            'DATA_PIPELINE_WORK' : data_pipeline_result\n",
    "         })\n",
    "\n",
    "    return results_records_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6551303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_work(env):\n",
    "    ''' two db's'''\n",
    "    try:\n",
    "        pd_dataframe_dict = get_db_active(f\"{env}_A\", os.getenv(\"SQL\"))\n",
    "        df = pd.DataFrame(pd_dataframe_dict)\n",
    "        display(df)\n",
    "        \n",
    "        pd_dataframe_dict = get_db_active(f\"{env}_B\", os.getenv(\"SQL\"))\n",
    "        df = pd.DataFrame(pd_dataframe_dict)\n",
    "        # print(df.head(10))\n",
    "        display(df)\n",
    "       \n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "\n",
    "    # finally:\n",
    "    #     logger.info(\"Job is being performed correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Validate if data is processing'''\n",
    "pd_dataframe_dict = get_db_work(\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f8a913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML('<pre style=\"background-color: #A52A2A; color: white;\">Job is being performed correctly</pre>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
